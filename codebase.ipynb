{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhruvprabhu/Tweet-Support-Score/blob/main/BTP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries\n"
      ],
      "metadata": {
        "id": "JPjJgNOILtYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "ELCWcZzlEgos",
        "outputId": "66a314b5-a20c-4ce3-a8e3-8120cd26ea3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    127\u001b[0m   )\n\u001b[1;32m    128\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ3j9Hu4Pp5v",
        "outputId": "5e144e09-8e67-48f1-c710-7d9109c9510c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.10.1-py2.py3-none-any.whl (421 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/421.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m419.8/421.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "corpus=open(\"/content/drive/MyDrive/Deep Learning/words.txt\").read().splitlines()"
      ],
      "metadata": {
        "id": "AJz7xUkunNEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random as rd\n",
        "import re\n",
        "import emoji\n",
        "import math\n",
        "import string\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "df= pd.read_csv('/content/drive/MyDrive/Deep Learning/annotations - Final_Dataset.csv')\n",
        "df = df[df['Reply_Class'] != 'Lang']"
      ],
      "metadata": {
        "id": "BmJIHCg_L5Ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hashtag_cache = {}\n",
        "\n",
        "def separate_hashtag_words(sentence):\n",
        "    # Check if the result is cached\n",
        "    sentence = sentence.lower()\n",
        "    if sentence in hashtag_cache:\n",
        "        return hashtag_cache[sentence]\n",
        "\n",
        "    # Calculate and cache the result\n",
        "    hashtags = re.findall(r'#(\\w+)', sentence.lower())\n",
        "    store = []\n",
        "    for hashtag in hashtags:\n",
        "        list_words = []\n",
        "        words = re.split(r'(\\d+)', hashtag)\n",
        "        def func():\n",
        "            for word in words:\n",
        "                list_words.append(word)\n",
        "            return list_words\n",
        "        word_list = func()\n",
        "        for element in word_list:\n",
        "            if element.isdigit():\n",
        "                store.append(element)\n",
        "            else:\n",
        "                new_word = element\n",
        "                word_separated, val1, val2 = '', 0, 0\n",
        "                while val2 < 30:\n",
        "                    for each_word in range(len(new_word), 0, -1):\n",
        "                        if new_word[val1:each_word] in corpus:\n",
        "                            word_separated = word_separated + ' ' + new_word[val1:each_word]\n",
        "                            val1 = each_word\n",
        "                            break\n",
        "                    val2 += 1\n",
        "                store.append(word_separated.strip())\n",
        "\n",
        "    # Format the output string\n",
        "    output_string = ' '.join(str(x) for x in store).title()\n",
        "    if output_string.endswith(\",\"):\n",
        "        output_string = output_string[:-1]  # Remove the trailing comma\n",
        "    # Cache the result\n",
        "    hashtag_cache[sentence] = output_string\n",
        "    return output_string.lower()"
      ],
      "metadata": {
        "id": "jF-2sQVjcpxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "output_string = separate_hashtag_words(\"#sanctionpakistan\")\n",
        "print(output_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWm3X9naS57z",
        "outputId": "dc2b2a0a-f5d5-4b03-9825-e94e2b788b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sanction pakistan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n"
      ],
      "metadata": {
        "id": "glQKCgfDN0Wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def pre_process_tweet(tweet):\n",
        "    # Remove \\n from the end of the sentence\n",
        "    tweet = tweet.strip('\\n')\n",
        "\n",
        "    # Convert hashtags to lowercase and separate hashtag words\n",
        "    tweet = separate_hashtag_words(tweet)\n",
        "\n",
        "    # Use regex to remove '@' only if followed by a word\n",
        "    tweet = re.sub(r'@(\\w+)', r'\\1', tweet)\n",
        "\n",
        "    # Remove any URL\n",
        "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
        "    tweet = re.sub(r\"www\\S+\", \"\", tweet)\n",
        "\n",
        "    # Convert emojis to their underscore-separated representation\n",
        "    tweet = emoji.demojize(tweet)\n",
        "    tweet = re.sub(r':(\\w+):', r'\\1 ', tweet)\n",
        "\n",
        "    # Convert underscore-separated words to space-separated words\n",
        "    tweet = re.sub(r'_', ' ', tweet)\n",
        "\n",
        "    # Convert dash-separated words to space-separated words\n",
        "    tweet = re.sub(r'-', ' ', tweet)\n",
        "\n",
        "    # Replace &amp with 'and'\n",
        "    tweet = tweet.replace('&amp','and')\n",
        "    tweet = tweet.replace('&AMP','and')\n",
        "    tweet = tweet.replace('â','')\n",
        "\n",
        "    # Replace U.S. with 'usa'\n",
        "    tweet = tweet.replace('U.S.', 'usa')\n",
        "    tweet = tweet.replace('US','usa')\n",
        "\n",
        "    # Remove colons from the end of the sentences (if any)\n",
        "    tweet = tweet.strip()\n",
        "    if tweet.endswith(':'):\n",
        "        tweet = tweet[:-1]\n",
        "\n",
        "    # Remove hash-tags symbols\n",
        "    tweet = tweet.replace('#', '')\n",
        "\n",
        "    # Convert every word to lowercase\n",
        "    tweet = tweet.lower()\n",
        "\n",
        "    # Remove punctuations\n",
        "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Trim extra spaces\n",
        "    tweet = \" \".join(tweet.split())\n",
        "\n",
        "    return tweet\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Example usage:\n",
        "# tweet = \"😙😚Here is an 'example' tweet! @user #example PM's us US #sanctionpakistan http://example.com  😂💀💀do a lot !!\"\n",
        "# processed_tweet = pre_process_tweet(tweet)\n",
        "# print(processed_tweet)\n"
      ],
      "metadata": {
        "id": "bIHYQsj-N55i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BQp2j1E7wyNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Text'] = df['Text'].apply(pre_process_tweet)\n",
        "df['Reply'] = df['Reply'].apply(pre_process_tweet)"
      ],
      "metadata": {
        "id": "zTO-gr9ZD1qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = []\n",
        "for i in df['Text']:\n",
        "    txt.append(pre_process_tweet(i))\n",
        "\n",
        "df['Text'] = txt"
      ],
      "metadata": {
        "id": "zKYlK7sQiW7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt"
      ],
      "metadata": {
        "id": "kVrjPCfii17u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = []\n",
        "for i in df['Reply']:\n",
        "    txt.append(pre_process_tweet(i))\n",
        "\n",
        "df['Reply'] = txt"
      ],
      "metadata": {
        "id": "2wh1yt2hivI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nKZ9D21JYrdh",
        "outputId": "9ef81cc1-f94f-47b5-b665-f901a0571151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pre_process_tweet(\"#SanctionPakistan\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOPa-rPUXuO2",
        "outputId": "fc9c0b71-aca0-4e3e-9f4c-d63d02e3304e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sanction pakistan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization and Padding\n"
      ],
      "metadata": {
        "id": "F1xxFtsvD8rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and Padding\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['Text'] + df['Reply'])\n",
        "\n",
        "max_sequence_length = 60\n",
        "\n",
        "sequences_text = tokenizer.texts_to_sequences(df['Text'])\n",
        "sequences_reply = tokenizer.texts_to_sequences(df['Reply'])\n",
        "\n",
        "padded_sequences_text = pad_sequences(sequences_text, maxlen=max_sequence_length, padding='post')\n",
        "padded_sequences_reply = pad_sequences(sequences_reply, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "\n",
        "\n",
        "reply_tokens = [tokenizer.index_word.get(token, '') for token in padded_sequences_reply[30]]\n",
        "print(\"Reply tokens:\", reply_tokens)\n",
        "print(padded_sequences_reply[30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UAyJejiEICN",
        "outputId": "5677afa5-7a91-47bb-825e-1fa75a977dd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reply tokens: ['sanctionpakistan', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[85  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Glove dictionary creation"
      ],
      "metadata": {
        "id": "BTTeO-iUEKM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "words = dict()\n",
        "\n",
        "def add_to_dict(d, filename):\n",
        "  with open(filename, 'r') as f:\n",
        "    for line in f.readlines():\n",
        "      line = line.split(' ')\n",
        "\n",
        "      try:\n",
        "        d[line[0]] = np.array(line[1:], dtype=float)\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "add_to_dict(words, '/content/drive/MyDrive/Deep Learning/glove.twitter.27B.50d.txt')\n",
        "words"
      ],
      "metadata": {
        "id": "TKYZ3S4pEO5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create Embeddings"
      ],
      "metadata": {
        "id": "CiSwk_97ESin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Function to get embeddings for sequences\n",
        "def get_embeddings(sequences, word_dict):\n",
        "    embedding_dim = 50\n",
        "    # embedding_dim = len(next(iter(word_dict.values())))  # Dimension of GloVe embeddings\n",
        "    num_words = len(tokenizer.word_index) + 1  # Adding 1 for padding token (index 0)\n",
        "\n",
        "    # Initialize embeddings matrix\n",
        "    embeddings_matrix = np.zeros((len(sequences), max_sequence_length, embedding_dim))\n",
        "\n",
        "    # Iterate over sequences\n",
        "    for i, seq in enumerate(sequences):\n",
        "        # Iterate over tokens in each sequence\n",
        "        for j, token_index in enumerate(seq):\n",
        "            if token_index != 0:  # Skip padding token\n",
        "                word = tokenizer.index_word.get(token_index)\n",
        "                if word in word_dict:\n",
        "                    embeddings_matrix[i, j, :] = word_dict[word]\n",
        "                else:\n",
        "                    # If word not in GloVe dictionary, use zero vector\n",
        "                    embeddings_matrix[i, j, :] = np.zeros(embedding_dim)\n",
        "\n",
        "    return embeddings_matrix\n",
        "\n",
        "# Get embeddings for padded_sequences_text and padded_sequences_reply\n",
        "embeddings_text = get_embeddings(padded_sequences_text, words)\n",
        "embeddings_reply = get_embeddings(padded_sequences_reply, words)\n",
        "\n",
        "# Print the shape of embeddings for verification\n",
        "# print(\"Shape of embeddings_text:\", embeddings_text.shape)\n",
        "# print(\"Shape of embeddings_reply:\", embeddings_reply.shape)\n",
        "\n",
        "print(embeddings_reply[0]) #1st word of 1st tweet text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bkDrxnQHi9A",
        "outputId": "366c69dd-016c-454b-ef92-4cce3d514c96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.2532   -0.014884  0.59371  ... -0.28932   0.45493   0.18659 ]\n",
            " [-0.45111   0.10386  -0.39865  ...  0.48346  -0.40463   0.57756 ]\n",
            " [ 0.30221   0.29845   0.42853  ...  0.057719  0.44477   0.013107]\n",
            " ...\n",
            " [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
            " [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
            " [ 0.        0.        0.       ...  0.        0.        0.      ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CREATION OF TRAIN, TEST, VAL"
      ],
      "metadata": {
        "id": "1mFxr3g6ABY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y=df['Reply_Class']\n"
      ],
      "metadata": {
        "id": "FaeMmuTAD44I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming you have your data stored in embeddings_text and embeddings_reply arrays\n",
        "# Split the data into training (80%), validation (10%), and test (10%) sets\n",
        "X_train_text, X_temp_text, X_val_text, X_test_text = train_test_split(embeddings_text, test_size=0.2, random_state=42)\n",
        "X_val_text, X_test_text = train_test_split(X_temp_text, test_size=0.5, random_state=42)\n",
        "\n",
        "X_train_reply, X_temp_reply, X_val_reply, X_test_reply = train_test_split(embeddings_reply, test_size=0.2, random_state=42)\n",
        "X_val_reply, X_test_reply = train_test_split(X_temp_reply, test_size=0.5, random_state=42)\n",
        "\n",
        "# Assuming you have labels stored in a variable y\n",
        "# Split the labels accordingly\n",
        "y_train, y_temp = train_test_split(y, test_size=0.2, random_state=42)\n",
        "y_val, y_test = train_test_split(y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convert the arrays to numpy arrays\n",
        "X_train_text = np.array(X_train_text)\n",
        "X_val_text = np.array(X_val_text)\n",
        "X_test_text = np.array(X_test_text)\n",
        "\n",
        "X_train_reply = np.array(X_train_reply)\n",
        "X_val_reply = np.array(X_val_reply)\n",
        "X_test_reply = np.array(X_test_reply)\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_val = np.array(y_val)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Print the shapes to verify\n",
        "print(\"X_train_text shape:\", X_train_text.shape)\n",
        "print(\"X_val_text shape:\", X_val_text.shape)\n",
        "print(\"X_test_text shape:\", X_test_text.shape)\n",
        "print(\"X_train_reply shape:\", X_train_reply.shape)\n",
        "print(\"X_val_reply shape:\", X_val_reply.shape)\n",
        "print(\"X_test_reply shape:\", X_test_reply.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_val shape:\", y_val.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "Nxz6YuygAI9B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "993f5bd1-31e7-4e36-da8a-aea8a699113e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 4, got 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-208778a852d4>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Assuming you have your data stored in embeddings_text and embeddings_reply arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Split the data into training (80%), validation (10%), and test (10%) sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX_train_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_temp_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mX_val_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_temp_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM MODEL"
      ],
      "metadata": {
        "id": "ouM0ftSF_95Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "\n",
        "# Define the input embedding (example embedding of a single sentence)\n",
        "embedding = embeddings_reply[0] # Example embedding of shape (60, 50)\n",
        "\n",
        "# Define the Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the LSTM layer to the model\n",
        "model.add(LSTM(64, input_shape=(60, 50)))\n",
        "\n",
        "# Compile the model (not necessary for this demonstration)\n",
        "model.compile(optimizer='adam', loss='mse')  # Any dummy optimizer and loss for demonstration purpose\n",
        "\n",
        "# Print the model summary\n",
        "print(model.summary())\n",
        "\n",
        "# Predict the intermediate output for the input embedding\n",
        "intermediate_output = model.predict(np.expand_dims(embedding, axis=0))\n",
        "\n",
        "# Print the shape of the intermediate output\n",
        "print(\"Shape of intermediate output:\", intermediate_output.shape)\n",
        "\n",
        "# Print the intermediate output itself\n",
        "print(\"Intermediate output:\")\n",
        "print(intermediate_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvCBfCSB7vPP",
        "outputId": "59e39a0d-2a00-4470-ac35-b217508fc97b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 64)                29440     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 29440 (115.00 KB)\n",
            "Trainable params: 29440 (115.00 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "1/1 [==============================] - 0s 401ms/step\n",
            "Shape of intermediate output: (1, 64)\n",
            "Intermediate output:\n",
            "[[ 6.86205531e-05 -6.04103889e-06  8.04972879e-05 -2.70733854e-05\n",
            "   5.62488785e-05 -4.27173181e-05 -1.26177227e-04 -4.11624678e-05\n",
            "  -7.58803726e-05  1.77580168e-05  4.34321373e-05  2.31117247e-05\n",
            "  -9.52698683e-05 -6.64584804e-05  2.39157962e-05 -8.82857858e-05\n",
            "   8.96582278e-05  2.58536766e-05 -3.29308568e-05  1.40438806e-05\n",
            "  -7.84855365e-05 -5.29215395e-05 -1.11455192e-05  4.62953685e-05\n",
            "  -8.30017743e-05 -1.96008637e-04 -1.71533873e-04  5.34708124e-05\n",
            "   1.17577467e-04 -1.01700927e-04  5.82420253e-05  5.35568834e-05\n",
            "   1.80151692e-05 -1.27532927e-04  6.62004531e-05  8.21997528e-05\n",
            "  -6.43000094e-05  6.30335053e-05 -1.72822365e-05 -1.33028647e-04\n",
            "   8.73810568e-05  8.27658514e-05 -2.72214911e-05 -6.68880512e-06\n",
            "   2.44542639e-06 -5.91495809e-05 -9.87368039e-05  6.58212230e-05\n",
            "   1.80805382e-05  1.93764528e-04  4.55504123e-05  6.60061123e-05\n",
            "  -1.09994711e-04 -2.75619350e-05  3.44241598e-05 -8.85055633e-05\n",
            "  -3.93848459e-05 -1.11239504e-04 -3.30503610e-07 -2.90395965e-05\n",
            "  -1.33269083e-07 -1.60826268e-04  7.25786449e-05  2.67043451e-05]]\n"
          ]
        }
      ]
    }
  ]
}
