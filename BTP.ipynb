{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kambojharsh/Tweet-Support-Classifier/blob/main/BTP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPjJgNOILtYA"
      },
      "source": [
        "# Importing libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELCWcZzlEgos",
        "outputId": "fb0dc69e-d96d-4a6b-b7b8-7d55782bc5b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ3j9Hu4Pp5v",
        "outputId": "ed22c55a-c7eb-47af-be11-03764be98d29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.10.1-py2.py3-none-any.whl (421 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/421.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.4/421.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m419.8/421.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.10.1\n",
            "Collecting wordninja\n",
            "  Downloading wordninja-2.0.0.tar.gz (541 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m541.6/541.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wordninja\n",
            "  Building wheel for wordninja (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordninja: filename=wordninja-2.0.0-py3-none-any.whl size=541530 sha256=6df91ef02d559f09d24266d1d18aa7798e0439ad6c81b07c2ea7b5e61b7f6ba4\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/44/3a/f2a5c1859b8b541ded969b4cd12d0a58897f12408f4f51e084\n",
            "Successfully built wordninja\n",
            "Installing collected packages: wordninja\n",
            "Successfully installed wordninja-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji\n",
        "!pip install wordninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmJIHCg_L5Ni"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random as rd\n",
        "import re\n",
        "import emoji\n",
        "import math\n",
        "import string\n",
        "import numpy as np\n",
        "import wordninja\n",
        "from gensim.models import KeyedVectors\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "df= pd.read_csv('/content/drive/MyDrive/DL/annotations - Final_Dataset.csv')\n",
        "df = df[df['Reply_Class'] != 'Lang']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glQKCgfDN0Wb"
      },
      "source": [
        "# Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def pre_process_tweet(tweet):\n",
        "    # Remove \\n from the end of the sentence\n",
        "    tweet = tweet.strip('\\n')\n",
        "\n",
        "    # Convert emojis to their descriptions\n",
        "    tweet = emoji.demojize(tweet)\n",
        "    # Add spaces between emoji descriptions\n",
        "    tweet = re.sub(r'(:\\w+:)', lambda x: x.group(0).replace(':', ' '), tweet)\n",
        "\n",
        "    # Remove any URL\n",
        "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
        "    tweet = re.sub(r\"www\\S+\", \"\", tweet)\n",
        "\n",
        "    # Convert underscore-separated words to space-separated words\n",
        "    tweet = re.sub(r'_', ' ', tweet)\n",
        "\n",
        "    # Convert dash-separated words to space-separated words\n",
        "    tweet = re.sub(r'-', ' ', tweet)\n",
        "\n",
        "    # Replace &amp with 'and'\n",
        "    tweet = tweet.replace('&amp','and')\n",
        "    tweet = tweet.replace('&AMP','and')\n",
        "\n",
        "    # Replace U.S. with 'usa'\n",
        "    tweet = tweet.replace('U.S.', 'usa')\n",
        "    tweet = tweet.replace('US','usa')\n",
        "    # Remove colons from the end of the sentences (if any)\n",
        "    tweet = tweet.strip()\n",
        "    if tweet.endswith(':'):\n",
        "        tweet = tweet[:-1]\n",
        "\n",
        "    # Split tweet into words\n",
        "    # words = tweet.split()\n",
        "\n",
        "    # # Iterate over words and replace hashtag words\n",
        "    # for i, word in enumerate(words):\n",
        "    #     if word.startswith('#'):\n",
        "    #         words[i] = separate_hashtag_words(word)\n",
        "\n",
        "    # tweet = ' '.join(words)\n",
        "    # tweet = re.sub(r'#\\w+', lambda x: separate_hashtag_words(x.group()), tweet)\n",
        "    # Remove hash-tags symbols and add spaces between words\n",
        "    # tweet = re.sub(r'#(\\w+)', r' \\1', tweet)\n",
        "\n",
        "    # Convert every word to lowercase\n",
        "    tweet = tweet.lower()\n",
        "\n",
        "\n",
        "    tweet = tweet.replace('#', '')\n",
        "    # Remove punctuations\n",
        "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Trim extra spaces\n",
        "    tweet = \" \".join(tweet.split())\n",
        "\n",
        "    return tweet\n",
        "\n",
        "# Example usage:\n",
        "tweet = \"#sanctionpakistan\"\n",
        "processed_tweet = pre_process_tweet(tweet)\n",
        "print(processed_tweet)\n",
        "tweet = \"üòôüòöHere is an 'example' tweet! @user #example PM's us US  #sanctionpakistan http://example.com  üòÇüíÄüíÄdo a lot !!\"\n",
        "processed_tweet = pre_process_tweet(tweet)\n",
        "print(processed_tweet)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSNgWjo5qQWn",
        "outputId": "f75f4c66-082d-4461-f67e-d48f712f7088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sanctionpakistan\n",
            "kissing face with smiling eyes kissing face with closed eyes here is an example tweet user example pms us usa sanctionpakistan face with tears of joy skull skull do a lot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTO-gr9ZD1qW"
      },
      "outputs": [],
      "source": [
        "df['Text'] = df['Text'].apply(pre_process_tweet)\n",
        "df['Reply'] = df['Reply'].apply(pre_process_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to seperate words\n",
        "df['Text'] = df['Text'].apply(lambda x: ' '.join(wordninja.split(x)))\n",
        "df['Reply'] = df['Reply'].apply(lambda x: ' '.join(wordninja.split(x)))"
      ],
      "metadata": {
        "id": "OB9h0QgKyQM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOPa-rPUXuO2",
        "outputId": "9fa6ba71-a6d2-4366-df8a-13764f2a81f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sanction pakistan united nations united nations high commissioner for human rights united nations general assembly international court of justice international courts\n"
          ]
        }
      ],
      "source": [
        "print(df['Reply'][11])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1xxFtsvD8rn"
      },
      "source": [
        "# Tokenization and Padding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UAyJejiEICN",
        "outputId": "a2766650-933e-48bf-9b71-2af5782293a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reply tokens: ['sanction', 'pakistan', 'united', 'nations', 'united', 'nations', 'high', 'commissioner', 'for', 'human', 'rights', 'united', 'nations', 'general', 'assembly', 'international', 'court', 'of', 'justice', 'international', 'courts', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[  54   19  372  794  372  794 1350 4423   11  469  564  372  794  897\n",
            " 4424  495 2497    4  531  495 3122    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "# Tokenization and Padding\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['Text'] + df['Reply'])\n",
        "\n",
        "max_sequence_length = 60\n",
        "\n",
        "sequences_text = tokenizer.texts_to_sequences(df['Text'])\n",
        "sequences_reply = tokenizer.texts_to_sequences(df['Reply'])\n",
        "\n",
        "padded_sequences_text = pad_sequences(sequences_text, maxlen=max_sequence_length, padding='post')\n",
        "padded_sequences_reply = pad_sequences(sequences_reply, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "\n",
        "\n",
        "reply_tokens = [tokenizer.index_word.get(token, '') for token in padded_sequences_reply[11]]\n",
        "print(\"Reply tokens:\", reply_tokens)\n",
        "print(padded_sequences_reply[11])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTTeO-iUEKM5"
      },
      "source": [
        "# Glove dictionary creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKYZ3S4pEO5k"
      },
      "outputs": [],
      "source": [
        "\n",
        "words = dict()\n",
        "\n",
        "def add_to_dict(d, filename):\n",
        "  with open(filename, 'r') as f:\n",
        "    for line in f.readlines():\n",
        "      line = line.split(' ')\n",
        "\n",
        "      try:\n",
        "        d[line[0]] = np.array(line[1:], dtype=float)\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "add_to_dict(words, '/content/drive/MyDrive/DL/glove.twitter.27B.50d.txt')\n",
        "words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiSwk_97ESin"
      },
      "source": [
        "#Create Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bkDrxnQHi9A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfa83398-92aa-4259-bec7-fc5b33e71de1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2864, 60, 50)\n",
            "(2864, 60, 50)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Function to get embeddings for sequences\n",
        "def get_embeddings(sequences, word_dict):\n",
        "    embedding_dim = 50\n",
        "    # embedding_dim = len(next(iter(word_dict.values())))  # Dimension of GloVe embeddings\n",
        "    num_words = len(tokenizer.word_index) + 1  # Adding 1 for padding token (index 0)\n",
        "\n",
        "    # Initialize embeddings matrix\n",
        "    embeddings_matrix = np.zeros((len(sequences), max_sequence_length, embedding_dim))\n",
        "\n",
        "    # Iterate over sequences\n",
        "    for i, seq in enumerate(sequences):\n",
        "        # Iterate over tokens in each sequence\n",
        "        for j, token_index in enumerate(seq):\n",
        "            if token_index != 0:  # Skip padding token\n",
        "                word = tokenizer.index_word.get(token_index)\n",
        "                if word in word_dict:\n",
        "                    embeddings_matrix[i, j, :] = word_dict[word]\n",
        "                else:\n",
        "                    # If word not in GloVe dictionary, use zero vector\n",
        "                    embeddings_matrix[i, j, :] = np.zeros(embedding_dim)\n",
        "\n",
        "    return embeddings_matrix\n",
        "\n",
        "# Get embeddings for padded_sequences_text and padded_sequences_reply\n",
        "embeddings_text = get_embeddings(padded_sequences_text, words)\n",
        "embeddings_reply = get_embeddings(padded_sequences_reply, words)\n",
        "\n",
        "# Print the shape of embeddings for verification\n",
        "# print(\"Shape of embeddings_text:\", embeddings_text.shape)\n",
        "# print(\"Shape of embeddings_reply:\", embeddings_reply.shape)\n",
        "\n",
        "print(embeddings_reply.shape)\n",
        "print(embeddings_text.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mFxr3g6ABY_"
      },
      "source": [
        "# CREATION OF TRAIN, TEST, VAL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=df['Reply_Class']"
      ],
      "metadata": {
        "id": "qBiy84g2Zywd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nxz6YuygAI9B"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming you have your data stored in embeddings_text and embeddings_reply arrays\n",
        "# Split the data into training (80%), validation (10%), and test (10%) sets\n",
        "X_train_text, X_temp_text = train_test_split(embeddings_text, test_size=0.2, random_state=42)\n",
        "X_val_text, X_test_text = train_test_split(X_temp_text, test_size=0.5, random_state=42)\n",
        "\n",
        "X_train_reply, X_temp_reply = train_test_split(embeddings_reply, test_size=0.2, random_state=42)\n",
        "X_val_reply, X_test_reply = train_test_split(X_temp_reply, test_size=0.5, random_state=42)\n",
        "\n",
        "# Assuming you have labels stored in a variable y\n",
        "# Split the labels accordingly\n",
        "y_train, y_temp = train_test_split(y, test_size=0.2, random_state=42)\n",
        "y_val, y_test = train_test_split(y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convert the arrays to numpy arrays\n",
        "X_train_text = np.array(X_train_text)\n",
        "X_val_text = np.array(X_val_text)\n",
        "X_test_text = np.array(X_test_text)\n",
        "\n",
        "X_train_reply = np.array(X_train_reply)\n",
        "X_val_reply = np.array(X_val_reply)\n",
        "X_test_reply = np.array(X_test_reply)\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_val = np.array(y_val)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Print the shapes to verify\n",
        "print(\"X_train_text shape:\", X_train_text.shape)\n",
        "print(\"X_val_text shape:\", X_val_text.shape)\n",
        "print(\"X_test_text shape:\", X_test_text.shape)\n",
        "print(\"X_train_reply shape:\", X_train_reply.shape)\n",
        "print(\"X_val_reply shape:\", X_val_reply.shape)\n",
        "print(\"X_test_reply shape:\", X_test_reply.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_val shape:\", y_val.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouM0ftSF_95Q"
      },
      "source": [
        "#LSTM MODEL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Concatenate, Flatten, Input"
      ],
      "metadata": {
        "id": "wNXiPpgQ89Yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_shape):\n",
        "    # Define input layers for text and reply embeddings\n",
        "    text_input = Input(shape=input_shape)  # Assuming input shape is (60, 50) for text embeddings\n",
        "    reply_input = Input(shape=input_shape)  # Assuming input shape is (60, 50) for reply embeddings\n",
        "\n",
        "    # # LSTM layer for text\n",
        "    lstm_text = LSTM(64, return_sequences=True)(text_input)\n",
        "\n",
        "    # # LSTM layer for reply\n",
        "    lstm_reply = LSTM(64, return_sequences=True)(reply_input)\n",
        "\n",
        "    # # Concatenate the outputs of both LSTM layers\n",
        "    concatenated = Concatenate(axis=1)([lstm_text, lstm_reply])\n",
        "\n",
        "    # # # Flatten the concatenated output\n",
        "    flattened = Flatten()(concatenated)\n",
        "\n",
        "    # # # Dense layer with softmax activation for 3 classes\n",
        "    output = Dense(3, activation='softmax')(lstm_text)\n",
        "\n",
        "    model = Model(inputs=[text_input, reply_input], outputs=output)\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X1UUjy7I7LOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, X_train, y_train, X_val, y_val, epochs=1, batch_size=32):\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Fit the model\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "    return history"
      ],
      "metadata": {
        "id": "cAaRySad7dhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test, batch_size=32):\n",
        "    # Evaluate the model on test data\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
        "    print(\"Test Loss:\", test_loss)\n",
        "    print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "jeEVnjxC7nBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (60, 50)  # Assuming input shape is (60, 50) for text and reply embeddings\n",
        "model = build_model(input_shape)\n",
        "# model.summary()\n",
        "history = train_model(model, X_train_text, y_train, X_val_text, y_val, 1, 32)\n",
        "evaluate_model(model , X_test_text, y_test, batch_size=32)"
      ],
      "metadata": {
        "id": "F3vAE-bo7qE-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}